# ä»0å¼€å§‹çˆ¬ç½‘ç«™

> windowsä¸‹ä»é›¶å¼€å§‹çˆ¬å–bossç›´è˜ä¸Šæ‰€æœ‰å®æ³¢åœ°åŒºçš„æä¾›javaå²—ä½çš„å…¬å¸ã€‚
>
> æœ¬æ–‡ä¸»è¦è®²çš„æ˜¯åœ¨ä¼šç”¨pythonå†™helloWorldçš„æƒ…å†µä¸‹å¦‚ä½•ä¸€æ­¥ä¸€æ­¥å…·ä½“å»å®ç°è¿™ä¸ªåŠŸèƒ½ã€‚
>
> å¦‚æœä¸æƒ³å¬é‚£ä¹ˆå¤šåºŸè¯å¯ä»¥ç›´æ¥ä¸‹è½½æºç ã€‚



1. ä¸‹è½½ pythonæœ€æ–°ç‰ˆæœ¬ï¼Œpyhton-3.6.5

2. IDEé€‰æ‹©PyCharm

3. `pip install scrapy` è¿™é‡Œwindowsæ— æ³•ç›´æ¥å®‰è£…ï¼Œä¼šæŠ¥é”™```building 'twisted.test.raiser' extension    error: Microsoft Visual C++ 14.0 is required. Get it with "Microsoft VisualC++ Build Tools": http://landinghub.visualstudio.com/visual-cpp-build-tools``` 

   è§£å†³æ–¹æ³•ï¼š

   1. http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted åœ¨è¿™ä¸ªåœ°å€ä¸‹è½½ç›¸åº”çš„whlç‰ˆæœ¬ä¿¡æ¯ï¼Œå…¶ä¸­cpåé¢å°±æ˜¯ä¾èµ–çš„Pythonç‰ˆæœ¬ã€‚ amd64è¡¨ç¤º64ä½æ“ä½œç³»ç»Ÿã€‚ä¸‹è½½åå°†æ–‡ä»¶æ”¾åˆ°Dç›˜ä¸‹ï¼Œä½¿ç”¨**pip install d:\Twisted-17.1.0-cp36-cp36m-win_amd64.whl å®‰è£…ã€‚**
   2. `pip install scrapy`

4. å‘½ä»¤è¡Œä¸­cdåˆ°ç›®æ ‡ç›®å½•æ‰§è¡Œ`scrapy startproject é¡¹ç›®åç§°`ï¼Œæˆ‘æ‰§è¡Œçš„æ˜¯`crapy startproject boss_grabbing`

5. PyCharm æ‰“å¼€è¯¥é¡¹ç›®

   ç›®å½•å±‚çº§

   ```
   boss_grabbing
       â”‚  items.py
       â”‚  middlewares.py
       â”‚  pipelines.py
       â”‚  settings.py
       â”‚  __init__.py
       â”‚
       â”œâ”€spiders
       â”‚  â”‚  company.py(æ‰‹åŠ¨æ–°å»ºï¼Œçˆ¬è™«å®ç°)
       â”‚  â”‚  __init__.py
       â”‚  â”‚
       â”‚  â””â”€__pycache__
       â”‚          company.cpython-36.pyc
       â”‚          __init__.cpython-36.pyc
       â”‚
       â”œâ”€startï¼ˆæ‰‹åŠ¨æ–°å»ºï¼Œç¨‹åºå…¥å£ï¼‰
       â”‚  â”‚  boss_start.py
       â”‚  â”‚  __init__.py
       â”‚  â”‚
       â”‚  â””â”€__pycache__
       â”‚          boss_start.cpython-36.pyc
       â”‚          __init__.cpython-36.pyc
       â”‚
       â””â”€__pycache__
               settings.cpython-36.pyc
               __init__.cpython-36.pyc
   ```

6. å…¶ä¸­`company.py`æ˜¯çˆ¬è™«ä¸»ç¨‹åºï¼Œitem.pyä¸ºè‡ªå®šä¹‰çš„æ•°æ®æ ¼å¼ï¼Œpipelineä¼šå¯¹itemè¿›è¡Œå¤„ç†ã€‚ä»¥æˆ‘ä¸æ±‚æ­£ç¡®çš„ç†è§£æµç¨‹å°±æ˜¯ï¼š**`boss_start.py`å¦‚ä½•å¯åŠ¨äº†`scrapy`ï¼Œ`scrapy`è¯»å–`BossSpider.py`çš„`start_urls`å–å‘é€è¯·æ±‚ï¼Œå¯¹äºè¿”å›çš„æ•°æ®è°ƒç”¨`BossSpider.py`çš„`parse()`æ–¹æ³•æ¥å¤„ç†ï¼Œåœ¨`parse()`æ–¹æ³•ä¸­æˆ‘ä»¬å¯ä»¥ç»„è£…`item`ï¼Œç„¶å`pipeline.py`å°±ä¼šè¿­ä»£åœ°å¯¹itemè¿›è¡Œå¤„ç†ã€‚**

   - æˆ‘ä»¬æ¥å†™ä¸€ä¸ªéœ€è¦çš„`company.py`

     ```python
     import scrapy

     class BossSpider(scrapy.Spider):
         # è¿™é‡Œæ˜¯å°†çˆ¬è™«å®šä¹‰ä¸ºscrapy.Spiderè¿™ä¸ªç±»ä¸‹çš„ä¸€ä¸ªå®ä¾‹ã€‚
         # Spiderè¿™ä¸ªç±»å®šä¹‰äº†çˆ¬è™«çš„å¾ˆå¤šåŸºæœ¬åŠŸèƒ½ï¼Œæˆ‘ä»¬ç›´æ¥å®ä¾‹åŒ–å°±å¥½ï¼Œ
         # çœå´äº†å¾ˆå¤šé‡å†™æ–¹æ³•çš„éº»çƒ¦ã€‚
         name = 'bossZhiPin'
         #è¿™æ˜¯çˆ¬è™«çš„åå­—ï¼Œè¿™ä¸ªéå¸¸é‡è¦ã€‚
         start_urls = ['https://www.zhipin.com/c101210400-p100101/h_101210400/?page=1']
         #è¿™æ˜¯çˆ¬è™«å¼€å§‹å¹²æ´»çš„åœ°å€ï¼Œå¿…é¡»æ˜¯ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ã€‚
      
         def parse(self, response):
             titles = response.xpath("//body").extract()#xpathæ¥è·å–å…ƒç´ 
         #çˆ¬è™«æ”¶åˆ°ä¸Šé¢çš„åœ°å€åï¼Œå°±ä¼šå‘é€requestsè¯·æ±‚ï¼Œåœ¨æ”¶åˆ°æœåŠ¡å™¨è¿”å›çš„å†…å®¹åï¼Œå°±å°†å†…å®¹ä¼ é€’ç»™parseå‡½æ•°ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬é‡å†™å‡½æ•°ï¼Œè¾¾åˆ°æˆ‘ä»¬æƒ³è¦çš„åŠŸèƒ½ã€‚
             for context in titles:
                 #çœ‹ä¸€ä¸‹æ‹¿åˆ°çš„é¡µé¢body
                 print(context)
     ```

   - æ–°å»ºä¸€ä¸ªåä¸º`start`çš„packageï¼Œç„¶åå†™ä¸ª`boss_start.py`

     ```python
     #!/usr/bin/python
     #coding:utf-8

     from scrapy import cmdline

     cmdline.execute("scrapy crawl bossZhiPin".split())#å¯¹åº”company.pyä¸­çš„name
     ```

     â€‹

7. è¿è¡Œ`boss_start.py`æ—¶æŠ¥`no module named win32api`ï¼Œè¿™æ—¶éœ€è¦è¿è¡Œ`pip install pypiwin32`ï¼Œç„¶åå†æ¬¡è¿è¡Œå³å¯ã€‚

8. æŸ¥çœ‹bodyå…ƒç´ ï¼Œæˆ‘éœ€è¦çš„å†…å®¹æ˜¯

   ```html
   <div class="company-text">
       <h3 class="name"><a href="/gongsi/872756.html" ka="search_list_company_1_custompage" target="_blank">å¤©å¹¿æ±‡é€š</a></h3>
       <p>æ•°æ®æœåŠ¡<em class="vline"></em>100-499äºº</p>
   </div>
   ```

   å› æ­¤ä¿®æ”¹`company.py`ä¸­çš„parseæ–¹æ³•

   ```python
   def parse(self, response):
           titles = response.xpath("//div[@class='company-text']").extract()#xpathæ¥è·å–å…ƒç´ 
       #çˆ¬è™«æ”¶åˆ°ä¸Šé¢çš„åœ°å€åï¼Œå°±ä¼šå‘é€requestsè¯·æ±‚ï¼Œåœ¨æ”¶åˆ°æœåŠ¡å™¨è¿”å›çš„å†…å®¹åï¼Œå°±å°†å†…å®¹ä¼ é€’ç»™parseå‡½æ•°ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬é‡å†™å‡½æ•°ï¼Œè¾¾åˆ°æˆ‘ä»¬æƒ³è¦çš„åŠŸèƒ½ã€‚
           for context in titles:
               #çœ‹ä¸€ä¸‹æ‹¿åˆ°çš„é¡µé¢body
               print(context)
   ```

9. è¿™æ—¶æ§åˆ¶å°è¿è¡Œè¾“å‡ºä¸º

   ```html
   <div class="company-text">
   	<h3 class="name"><a href="/gongsi/872756.html" ka="search_list_company_1_custompage" target="_blank">å¤©å¹¿æ±‡é€š</a></h3>
   	<p>æ•°æ®æœåŠ¡<em class="vline"></em>100-499äºº</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/1148153.html" ka="search_list_company_2_custompage" target="_blank">é­…æ´›è¿ª</a></h3>
   	<p>è®¡ç®—æœºè½¯ä»¶<em class="vline"></em>0-20äºº</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/1282528.html" ka="search_list_company_3_custompage" target="_blank">æ¸…è½¦</a></h3>
   	<p>æ±½è½¦ç”Ÿäº§<em class="vline"></em>20-99äºº</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/1714884.html" ka="search_list_company_4_custompage" target="_blank">å®æ³¢æ™®å¤©ä¿¡æ¯æŠ€æœ¯</a></h3>
   	<p>é€šä¿¡/ç½‘ç»œè®¾å¤‡<em class="vline"></em>500-999äºº</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/424608.html" ka="search_list_company_5_custompage" target="_blank">ä¸œè“</a></h3>
   	<p>è®¡ç®—æœºè½¯ä»¶<em class="vline"></em>20-99äºº</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/113231.html" ka="search_list_company_6_custompage" target="_blank">æµ™æ±Ÿé‡‘å¦®å„¿</a></h3>
   	<p>ç§»åŠ¨äº’è”ç½‘<em class="vline"></em>Bè½®<em class="vline"></em>100-499äºº</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/1179487.html" ka="search_list_company_7_custompage" target="_blank">é‡‘è–‡</a></h3>
   	<p>äº’è”ç½‘<em class="vline"></em>100-499äºº</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/1671813.html" ka="search_list_company_8_custompage" target="_blank">æ‚¬å‰‘ç½‘ç»œç§‘æŠ€æœ‰é™å…¬å¸</a></h3>
   	<p>ä¿¡æ¯å®‰å…¨<em class="vline"></em>0-20äºº</p>
   </div>
   ```

10. ç°åœ¨å·²ç»æœ‰å¾ˆå¤šæ•°æ®äº†ï¼Œæ¥ä¸‹æ¥éƒ½è¦è€ƒè™‘æ€ä¹ˆä¿å­˜çš„é—®é¢˜äº†ã€‚æ˜¯æ—¶å€™å¼€å§‹ç ”ç©¶item.pyçš„ä½œç”¨äº†ã€‚

  >Item å¯¹è±¡æ˜¯ç§ç®€å•çš„å®¹å™¨ï¼Œä¿å­˜äº†çˆ¬å–åˆ°å¾—æ•°æ®ã€‚å…¶æä¾›äº†[ç±»ä¼¼äºè¯å…¸(dictionary-like)çš„API](http://docs.python.org/library/stdtypes.html#dict)ä»¥åŠç”¨äºå£°æ˜å¯ç”¨å­—æ®µçš„ç®€å•è¯­æ³•ã€‚

11. æˆ‘åœ¨ä¸ååˆ†ç†è§£itemçš„æƒ…å†µä¸‹æŒ‰ç…§ç¤ºä¾‹å†™äº†æˆ‘éœ€è¦çš„item

    ```python
    # -*- coding: utf-8 -*-

    # Define here the models for your scraped items
    #
    # See documentation in:
    # https://doc.scrapy.org/en/latest/topics/items.html

    import scrapy
    class BossGrabbingItem(scrapy.Item):
        # define the fields for your item here like:
        name = scrapy.Field()#ç­‰å·å·¦è¾¹å°±æ˜¯æˆ‘ä»¬è¦å®šä¹‰çš„å­—æ®µ
        url = scrapy.Field()
        area = scrapy.Field()
        size = scrapy.Field()
        finance = scrapy.Field()
        description = scrapy.Field()
        pass

    ```

    ç­‰å·å·¦è¾¹å°±æ˜¯æˆ‘ä»¬è¦å®šä¹‰çš„å­—æ®µã€‚

12. ç„¶ååœ¨`company.py`çš„`parse()`æ–¹æ³•ä¸­ç»„è£…item

    ```python
       def parse(self, response):
            item = BossGrabbingItem()
            titles = response.xpath("//div[@class='company-text']").extract()

            for context in titles:
                item['name'] = re.findall('_blank">.*</a>', context)[0][8:-4]
                item['url'] = re.findall('/[^/]*html', context)[0][1:-5]
                item['area'] = re.findall('<p>[^<]*<em', context)[0][3:-3]
                finance = re.findall('em>.*<em', context)
                if len(finance) > 0:
                    item['finance'] = finance[0][3:-3]
                else:
                    item['finance'] = ''
                item['size'] = re.findall('em>[^<]*</p', context)[0][3:-3]
    		    print(item['name']+":"+item['url']+":"+item['area']+""+item['size']+":"+item['finance'])
    ```

    è¿™æ—¶è¿è¡Œåè¾“å‡ºæ˜¯

    ```
    é­…æ´›è¿ª:1148153:è®¡ç®—æœºè½¯ä»¶:0-20äºº:

    å¤©å¹¿æ±‡é€š:872756:æ•°æ®æœåŠ¡:100-499äºº:

    å®æ³¢æ™®å¤©ä¿¡æ¯æŠ€æœ¯:1714884:é€šä¿¡/ç½‘ç»œè®¾å¤‡:500-999äºº:

    AmarSoft:566493:äº’è”ç½‘é‡‘è:1000-9999äºº:å·²ä¸Šå¸‚

    å¥¥æ—ç§‘æŠ€:33637:äº’è”ç½‘:100-499äºº:Aè½®

    ä¸œè“:424608:è®¡ç®—æœºè½¯ä»¶:20-99äºº:

    æµ™æ±Ÿé‡‘å¦®å„¿:113231:ç§»åŠ¨äº’è”ç½‘:100-499äºº:Bè½®

    å—è¾¹é™€èº:1440691:è®¡ç®—æœºæœåŠ¡:0-20äºº:å¤©ä½¿è½®

    è…¾äº‘äº’è”:1742286:ç§»åŠ¨äº’è”ç½‘:20-99äºº:ä¸éœ€è¦èèµ„

    é‡‘è–‡:1179487:äº’è”ç½‘:100-499äºº:

    ç‰›å§ç§‘æŠ€:1634553:è®¡ç®—æœºè½¯ä»¶:0-20äºº:ä¸éœ€è¦èèµ„
    ```

13. ç°åœ¨å·²ç»ååˆ†æ¥è¿‘æˆ‘æƒ³è¦çš„å†…å®¹äº†ï¼Œç°åœ¨è¦å­¦ä¹ æ€ä¹ˆä¿å­˜æ•°æ®äº†ã€‚ä¸€å¼€å§‹æƒ³ç€æ˜¯é€šè¿‡æ–‡æœ¬å­˜å‚¨ï¼Œä½†æ˜¯æ–‡æœ¬çš„å»é‡æ˜¯ä¸ªéº»çƒ¦äº‹ï¼Œè¿œæ²¡æœ‰æ“ä½œæ•°æ®åº“æ¥å¾—ç®€å•ã€‚æ‰€ä»¥æœ€åè¿˜æ˜¯é‡‡ç”¨sqlite3è¿›è¡Œå­˜å‚¨ã€‚å› ä¸ºpythonè‡ªå¸¦sqlite3çš„åº“ğŸ˜„

14. ç½‘ä¸Šè¾¹æœè¾¹å­¦ï¼Œå†™å‡ºæ¥äº†`pipeline.py`ï¼Œå†™å®Œåè®°å¾—åœ¨`setting.py`ä¸­æ‰“å¼€è¿™æ®µæ³¨é‡Š

    ```python
    # Configure item pipelines
    # See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
    ITEM_PIPELINES = {
       'boss_grabbing.pipelines.BossGrabbingPipeline': 300,#è¿™ä¸ªè²Œä¼¼åœ¨å¤šä¸ªpipelineæ—¶æ‰æœ‰æ•ˆæœï¼Œæ•°å€¼å°çš„ä¼˜å…ˆ
    }
    ```

    ä¸‹é¢å°±æ˜¯`pipeline.py`

    ```python
    # -*- coding: utf-8 -*-

    # Define your item pipelines here
    #
    # Don't forget to add your pipeline to the ITEM_PIPELINES setting
    # See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html
    import os

    import sqlite3
    class BossGrabbingPipeline(object):

        # æ‰“å¼€æ•°æ®åº“
        def open_spider(self, spider):
            db_name = spider.settings.get('SQLITE_DB_NAME', 'sqlite3.db')
            self.db_conn = sqlite3.connect(db_name)
            self.db_cur = self.db_conn.cursor()

        # å…³é—­æ•°æ®åº“
        def close_spider(self, spider):
            self.db_conn.commit()
            self.db_conn.close()
    	
        #æ“ä½œitem
        def process_item(self, item, spider):
            print("process")
            count = self.select_db(item)[0][0]#è¿”å›çš„æ˜¯tupleï¼Œå®è·µè¯æ˜è¿™é‡Œæ˜¯ä¸ªäºŒç»´æ•°ç»„
            print("count:" + str(count))
            if count == 0:#ä¸é‡å¤çš„æ‰æ’å…¥
                self.insert_db(item)
            return item

        # æ’å…¥æ•°æ®
        def insert_db(self, item):
            values = (
                item['url'],
                item['name'],
                item['area'],
                item['finance'],
                item['size'],
            )

            sql = 'INSERT INTO company (id,name,area,finance,size) VALUES(?,?,?,?,?)'
            self.db_cur.execute(sql, values)

        # selectæ•°æ®
        def select_db(self, item):
            values = (
                item['url'],
            )

            sql = 'select count(1) from company where id=?'
            return self.db_cur.execute(sql, values).fetchall()
    ```

15. å¯¹äº†ï¼Œæ•°æ®åº“æˆ‘ç›´æ¥é€šè¿‡`sqliteStudio`åˆ›å»ºäº†ï¼Œå¹¶ä¸”å»ºäº†`company`è¡¨ã€‚ç°åœ¨æˆ‘å¼€å§‹æ”¹å†™`company.py`çš„`parseï¼ˆï¼‰`æ–¹æ³•

    ```python
        def parse(self, response):
            item = BossGrabbingItem()
            titles = response.xpath("//div[@class='company-text']").extract()
            for context in titles:
                item['name'] = re.findall('_blank">.*</a>', context)[0][8:-4]
                item['url'] = re.findall('/[^/]*html', context)[0][1:-5]
                item['area'] = re.findall('<p>[^<]*<em', context)[0][3:-3]
                finance = re.findall('em>.*<em', context)  ```
                if len(finance) > 0:
                    item['finance'] = finance[0][3:-3]
                else:
                    item['finance'] = ''
                item['size'] = re.findall('em>[^<]*</p', context)[0][3:-3]
                yield item#å—¯ï¼Œè¿™æ ·pipelineå°±ä¼šå–å¤„ç†äº†
    ```

16. è¿è¡ŒåæŸ¥çœ‹æ•°æ®åº“å‘ç°å·²ç»æœ‰è®°å½•äº†ï¼Œå•é¡µçš„æ•°æ®æŠ“å–å·²ç»å®Œæˆï¼ï¼æ¥ä¸‹æ¥å°±è¦è¿›è¡Œæ‰€æœ‰é¡µé¢çš„æ•°æ®æŠ“å–äº†ã€‚é¦–å…ˆç¡®å®šåˆ—è¡¨ä¸€å…±æœ‰10é¡µï¼Œé‚£ä¹ˆå°±æ˜¯è¦æŸ¥åæ¬¡å˜›ã€‚å¥½çš„ï¼Œå¾ªç¯æ„é€ ä¸€ä¸‹ç½‘å€ã€‚

    ```python
    import scrapy
    from scrapy import Request

    import re

    from boss_grabbing.items import BossGrabbingItem

    class BossSpider(scrapy.Spider):
        # start_urls = ['https://www.zhipin.com/c101210400-p100101/h_101210400/?page=2']
        base_url = 'https://www.zhipin.com/c101210400-p100101/h_101210400/?page='

        # æ–°å®ç°çš„æ–¹æ³•
        def start_requests(self):
            # æ„é€ ç½‘å€åˆ—è¡¨
            for i in range(1, 12):
                url = self.base_url + str(i)
                yield Request(url, self.parse)#å®ƒä¼šè¿­ä»£å»æŠ“å–

        def parse(self, response):
            item = BossGrabbingItem()
            titles = response.xpath("//div[@class='company-text']").extract()
            for context in titles:
                item['name'] = re.findall('_blank">.*</a>', context)[0][8:-4]
                item['url'] = re.findall('/[^/]*html', context)[0][1:-5]
                item['area'] = re.findall('<p>[^<]*<em', context)[0][3:-3]
                finance = re.findall('em>.*<em', context)
                if len(finance) > 0:
                    item['finance'] = finance[0][3:-3]
                else:
                    item['finance'] = ''
                item['size'] = re.findall('em>[^<]*</p', context)[0][3:-3]
                yield item
    ```

17. ç°åœ¨ï¼Œå·²ç»å¾—åˆ°äº†ä¸€ä¸ªå……æ»¡å…¬å¸çš„æ•°æ®è¡¨ã€‚ç„¶è€Œæ²¡æœ‰å…¬å¸ç»†èŠ‚ï¼Œæ‰€ä»¥è¿˜æ˜¯è¦ä»æ•°æ®è¡¨é‡Œselectå‡ºæ¥å†å»çˆ¬ã€‚ç…§ç€è‘«èŠ¦ç”»ç“¢ï¼Œæˆ‘å†™ä¸€ä¸ªçˆ¬å…¬å¸è¯¦æƒ…çš„æ–‡ä»¶

    ```python
    import sqlite3
    import re
    import scrapy
    from scrapy import Request

    class BossSpider(scrapy.Spider):
        name = 'bossDetail'
        base_url = 'https://www.zhipin.com/gongsi/'
        con = sqlite3.connect('sqlite3.db')
        cur = con.cursor()
        cur.execute('select * from(SELECT *,rowid FROM company where description is null) '
                    'where rowid < ((select min(rowid) from company where description is null) + 40)')#æ¯æ¬¡è¿­ä»£40ä¸ªå…¬å¸
        company_list = cur.fetchall()

        # è¿™æ˜¯çˆ¬è™«å¼€å§‹å¹²æ´»çš„åœ°å€ï¼Œå¿…é¡»æ˜¯ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ã€‚

        def start_requests(self):
            # æ„é€ ç½‘å€åˆ—è¡¨
            for i in self.company_list:
                url = self.base_url + str(i[0]) + ".html"
                yield Request(url, self.parse, meta={'id': i[0]})

        def parse(self, response):
            detail_content = response.xpath("//div[@class='detail-content']//div[@class='text fold-text']").extract()
            address_content = response.xpath("//div[@class='detail-content']//div[@class='job-location']").extract()
            print(detail_content)
            print(address_content)
            url = str(response.meta['id'])
            addresses = re.findall("a>[^<]+</div", str(address_content))
            description = ""
            descriptions = re.findall('text\">.+<a', str(detail_content))
            if len(descriptions) > 0:
                description = descriptions[0][6:-3]
            address_list = ""
            for address in addresses:
                address_list = address_list + ";" + address[2:-6]
            self.cur.execute('update company set description=?,address=? where id=?',
                             (description, str(address_list), int(url)))
            self.con.commit()

    ```

18. åŒæ ·å†åœ¨startæ–‡ä»¶å¤¹ä¸‹æ–°å¢`detail.python`ï¼Œå†…å®¹ä¸º

    ```python
    #!/usr/bin/python
    #coding:utf-8

    from scrapy import cmdline

    cmdline.execute("scrapy crawl bossDetail".split())
    ```

    è·‘äº†å‡ æ¬¡ä¹‹åå‘ç°æˆ‘çš„ipå·²ç»è¢«å°äº†...æ‰€ä»¥æˆ‘æƒ³ç€èƒ½ä¸èƒ½æœ‰ä¸ªdelayè®¾ç½®ã€‚ç½‘ä¸Šä¸€æŸ¥ï¼Œå‘ç°setting.pyä¸­æœ‰ä¸ªå€¼å¯ä»¥ä¿®æ”¹

    ```python
    # Configure a delay for requests for the same website (default: 0)
    # See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay
    # See also autothrottle settings and docs
    DOWNLOAD_DELAY = 3 #æŠŠè¿™ä¸ªæ³¨é‡Šæ‰“å¼€
    ```

    è¿™æ ·å°±æ˜¯3ç§’è¯·æ±‚ä¸€æ¬¡ï¼Œç„¶è€Œæˆ‘è¿˜æ˜¯ä¸çŸ¥é“å°ipçš„è§„åˆ™ï¼Œæ‰€ä»¥åˆ†äº†å¥½å‡ å¤©æŠŠå…¬å¸è·‘å®Œã€‚

19. æ©ï¼Œç°åœ¨å·²ç»å¾ˆå®Œç¾äº†ã€‚åªä¸è¿‡æŸ¥è¯¢åˆ—è¡¨å’ŒæŸ¥è¯¢å…¬å¸è¯¦æƒ…è¦åˆ†ä¸¤æ¬¡è¿è¡Œï¼Œç€å®æœ‰ç‚¹ä¸ä¼˜é›…ï¼Œä¸‹ä¸€æ­¥å°±æ˜¯åˆå¹¶ä»£ç ã€‚è¿™æ—¶å€™å°±å¾—é‡æ–°è€ƒè™‘ä¸€ä¸ªæ•°æ®å»é‡çš„é—®é¢˜ï¼Œå› ä¸ºå¤šä¸ªå²—ä½å¯¹åº”çš„å¯èƒ½æ˜¯åŒä¸€ä¸ªå…¬å¸ï¼Œæ‰€ä»¥åœ¨çˆ¬å–å…¬å¸è¯¦æƒ…ä¹‹å‰å°±è¦è¿›è¡Œåˆ¤æ–­ï¼Œä»¥è¾¾åˆ°å‡å°‘çˆ¬å–é¡µé¢çš„ç›®çš„ã€‚

20. è¿™æ ·çš„è¯æˆ‘å°±è¦åœ¨ç”Ÿæˆurlä¹‹å‰å»æ•°æ®åº“åˆ¤æ–­æ˜¯å¦å­˜åœ¨è¿™å®¶å…¬å¸ã€‚æ‰€ä»¥æˆ‘è¦åœ¨spideré‡Œæ“ä½œæ•°æ®åº“ã€‚å‚è€ƒäº†ä¸€äº›ä»£ç ï¼Œæˆ‘å°†æ•°æ®åº“ç›¸å…³æ“ä½œæŠ½ç¦»äº†å‡ºæ¥ã€‚åœ¨æ ¹ç›®å½•æ–°å»º`sqlite.py`

    ```python
    import sqlite3
    import time

    con = sqlite3.connect('sqlite3.db')
    cur = con.cursor()
    print("sqliteè¿æ¥æˆåŠŸ")

    class Sqlite(object):
        # æ£€æŸ¥å…¬å¸æ˜¯å¦å­˜åœ¨
        @classmethod
        def select_db(cls, company_id):
            sql = 'select count(1) from company where id=?'
            return cur.execute(sql, (company_id,)).fetchall()

        @classmethod
        def insert_db(cls, item):
            values = (
                item['url'],
                item['name'],
                item['area'],
                item['finance'],
                item['size'],
                item['description'],
                item['addresses'],
                time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            )

            sql = 'INSERT INTO company (id,name,area,finance,size,DESCRIPTION,ADDRESS,CREATE_TIME) VALUES(?,?,?,?,?,?,?,?)'
            cur.execute(sql, values)
            con.commit()
    ```

21. ç„¶åæŠŠ`detail.py`ä¸­çš„å†…å®¹æ”¹åˆ°`company.py` ä¸­

    ```python
    import scrapy
    from scrapy import Request

    import re

    from boss_grabbing.items import BossGrabbingItem
    from boss_grabbing.sqlite import Sqlite

    class BossSpider(scrapy.Spider):
        # è¿™é‡Œæ˜¯å°†çˆ¬è™«å®šä¹‰ä¸ºscrapy.Spiderè¿™ä¸ªç±»ä¸‹çš„ä¸€ä¸ªå®ä¾‹ã€‚
        # Spiderè¿™ä¸ªç±»å®šä¹‰äº†çˆ¬è™«çš„å¾ˆå¤šåŸºæœ¬åŠŸèƒ½ï¼Œæˆ‘ä»¬ç›´æ¥å®ä¾‹åŒ–å°±å¥½ï¼Œ
        # çœå´äº†å¾ˆå¤šé‡å†™æ–¹æ³•çš„éº»çƒ¦ã€‚
        name = 'bossZhiPin'
        # è¿™æ˜¯çˆ¬è™«çš„åå­—ï¼Œè¿™ä¸ªéå¸¸é‡è¦ã€‚
        # start_urls = ['https://www.zhipin.com/c101210400-p100101/h_101210400/?page=2']
        base_url = 'https://www.zhipin.com/c101210400-p100101/h_101210400/?page='

        # è¿™æ˜¯çˆ¬è™«å¼€å§‹å¹²æ´»çš„åœ°å€ï¼Œå¿…é¡»æ˜¯ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ã€‚

        def start_requests(self):
            # æ„é€ ç½‘å€åˆ—è¡¨
            for i in range(0, 2):
                url = self.base_url + str(i)
                yield Request(url, self.parse)

        def parse(self, response):
            titles = response.xpath("//div[@class='company-text']").extract()  # xpathçš„å†…å®¹ç½‘ä¸Šæœ
            # çˆ¬è™«æ”¶åˆ°ä¸Šé¢çš„åœ°å€åï¼Œå°±ä¼šå‘é€requestsè¯·æ±‚ï¼Œåœ¨æ”¶åˆ°æœåŠ¡å™¨è¿”å›çš„å†…å®¹åï¼Œå°±å°†å†…å®¹ä¼ é€’ç»™parseå‡½æ•°ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬é‡å†™å‡½æ•°ï¼Œè¾¾åˆ°æˆ‘ä»¬æƒ³è¦çš„åŠŸèƒ½ã€‚
            for context in titles:
                item = BossGrabbingItem()
                item['url'] = re.findall('/[^/]*html', context)[0][1:-5]
                print(item['url'])
                count = Sqlite.select_db(item['url'])[0][0]  # æ²¡æœ‰å…·ä½“ç ”ç©¶è¿‡ï¼Œé»˜è®¤è¿”å›çš„æ˜¯tupleç±»å‹ï¼Œç±»ä¼¼äºŒç»´æ•°ç»„
                if count > 0:
                    return
                item['name'] = re.findall('_blank">.*</a>', context)[0][8:-4]
                item['area'] = re.findall('<p>[^<]*<em', context)[0][3:-3]
                finance = re.findall('em>.*<em', context)
                if len(finance) > 0:
                    item['finance'] = finance[0][3:-3]
                else:
                    item['finance'] = ''
                item['size'] = re.findall('em>[^<]*</p', context)[0][3:-3]
                url = 'https://www.zhipin.com/gongsi/' + item['url'] + ".html"
                yield Request(url, callback=self.get_detail, meta={'item': item})  # è¿™æ ·å°±ä¼šè°ƒç”¨get_detail()æ–¹æ³•ï¼Œå¹¶ä¸”æŠŠitemä¼ è¿‡å»

        # è·å–å…¬å¸çš„è¯¦æƒ…å’Œåœ°å€
        def get_detail(self, response):
            item = response.meta['item']
            detail_content = response.xpath("//div[@class='detail-content']//div[@class='text fold-text']").extract()
            address_content = response.xpath("//div[@class='detail-content']//div[@class='job-location']").extract()
            print(detail_content)
            print(address_content)
            addresses = re.findall("a>[^<]+</div", str(address_content))
            description = ""
            descriptions = re.findall('text\">.+<a', str(detail_content))
            if len(descriptions) > 0:
                description = descriptions[0][6:-2]
            address_list = ""
            for address in addresses:
                address_list = address_list + ";" + address[2:-5]
            item['description'] = description
            item['addresses'] = address_list[1:]  # å»æ‰ç¬¬ä¸€ä¸ªåˆ†å·
            yield item

    ```

22. åŒæ ·ï¼Œè¿˜è¦ä¿®æ”¹`pipelines.py`

    ```python
    from boss_grabbing.sqlite import Sqlite

    class BossGrabbingPipeline(object):

        def process_item(self, item, spider):
            print("process")
            count = Sqlite.select_db(item['url'])[0][0]
            print("count:" + str(count))
            if count == 0:
                Sqlite.insert_db(item)
            return item

    ```

23. è¿è¡Œ`boss_start.py`ï¼Œå¾—åˆ°äº†ä¸€ä¸ª173æ¡è®°å½•çš„æ•°æ®è¡¨ã€‚

24. ç»“æŸã€‚


