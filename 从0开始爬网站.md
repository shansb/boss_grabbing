# 从0开始爬网站

> windows下从零开始爬取boss直聘上所有宁波地区的提供java岗位的公司。
>
> 本文主要讲的是在会用python写helloWorld的情况下如何一步一步具体去实现这个功能。
>
> 如果不想听那么多废话可以直接下载源码。



1. 下载 python最新版本，pyhton-3.6.5

2. IDE选择PyCharm

3. `pip install scrapy` 这里windows无法直接安装，会报错```building 'twisted.test.raiser' extension    error: Microsoft Visual C++ 14.0 is required. Get it with "Microsoft VisualC++ Build Tools": http://landinghub.visualstudio.com/visual-cpp-build-tools``` 

   解决方法：

   1. http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted 在这个地址下载相应的whl版本信息，其中cp后面就是依赖的Python版本。 amd64表示64位操作系统。下载后将文件放到D盘下，使用**pip install d:\Twisted-17.1.0-cp36-cp36m-win_amd64.whl 安装。**
   2. `pip install scrapy`

4. 命令行中cd到目标目录执行`scrapy startproject 项目名称`，我执行的是`crapy startproject boss_grabbing`

5. PyCharm 打开该项目

   目录层级

   ```
   boss_grabbing
       │  items.py
       │  middlewares.py
       │  pipelines.py
       │  settings.py
       │  __init__.py
       │
       ├─spiders
       │  │  company.py(手动新建，爬虫实现)
       │  │  __init__.py
       │  │
       │  └─__pycache__
       │          company.cpython-36.pyc
       │          __init__.cpython-36.pyc
       │
       ├─start（手动新建，程序入口）
       │  │  boss_start.py
       │  │  __init__.py
       │  │
       │  └─__pycache__
       │          boss_start.cpython-36.pyc
       │          __init__.cpython-36.pyc
       │
       └─__pycache__
               settings.cpython-36.pyc
               __init__.cpython-36.pyc
   ```

6. 其中`company.py`是爬虫主程序，item.py为自定义的数据格式，pipeline会对item进行处理。以我不求正确的理解流程就是：**`boss_start.py`如何启动了`scrapy`，`scrapy`读取`BossSpider.py`的`start_urls`取发送请求，对于返回的数据调用`BossSpider.py`的`parse()`方法来处理，在`parse()`方法中我们可以组装`item`，然后`pipeline.py`就会迭代地对item进行处理。**

   - 我们来写一个需要的`company.py`

     ```python
     import scrapy

     class BossSpider(scrapy.Spider):
         # 这里是将爬虫定义为scrapy.Spider这个类下的一个实例。
         # Spider这个类定义了爬虫的很多基本功能，我们直接实例化就好，
         # 省却了很多重写方法的麻烦。
         name = 'bossZhiPin'
         #这是爬虫的名字，这个非常重要。
         start_urls = ['https://www.zhipin.com/c101210400-p100101/h_101210400/?page=1']
         #这是爬虫开始干活的地址，必须是一个可迭代对象。
      
         def parse(self, response):
             titles = response.xpath("//body").extract()#xpath来获取元素
         #爬虫收到上面的地址后，就会发送requests请求，在收到服务器返回的内容后，就将内容传递给parse函数。在这里我们重写函数，达到我们想要的功能。
             for context in titles:
                 #看一下拿到的页面body
                 print(context)
     ```

   - 新建一个名为`start`的package，然后写个`boss_start.py`

     ```python
     #!/usr/bin/python
     #coding:utf-8

     from scrapy import cmdline

     cmdline.execute("scrapy crawl bossZhiPin".split())#对应company.py中的name
     ```

     ​

7. 运行`boss_start.py`时报`no module named win32api`，这时需要运行`pip install pypiwin32`，然后再次运行即可。

8. 查看body元素，我需要的内容是

   ```html
   <div class="company-text">
       <h3 class="name"><a href="/gongsi/872756.html" ka="search_list_company_1_custompage" target="_blank">天广汇通</a></h3>
       <p>数据服务<em class="vline"></em>100-499人</p>
   </div>
   ```

   因此修改`company.py`中的parse方法

   ```python
   def parse(self, response):
           titles = response.xpath("//div[@class='company-text']").extract()#xpath来获取元素
       #爬虫收到上面的地址后，就会发送requests请求，在收到服务器返回的内容后，就将内容传递给parse函数。在这里我们重写函数，达到我们想要的功能。
           for context in titles:
               #看一下拿到的页面body
               print(context)
   ```

9. 这时控制台运行输出为

   ```html
   <div class="company-text">
   	<h3 class="name"><a href="/gongsi/872756.html" ka="search_list_company_1_custompage" target="_blank">天广汇通</a></h3>
   	<p>数据服务<em class="vline"></em>100-499人</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/1148153.html" ka="search_list_company_2_custompage" target="_blank">魅洛迪</a></h3>
   	<p>计算机软件<em class="vline"></em>0-20人</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/1282528.html" ka="search_list_company_3_custompage" target="_blank">清车</a></h3>
   	<p>汽车生产<em class="vline"></em>20-99人</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/1714884.html" ka="search_list_company_4_custompage" target="_blank">宁波普天信息技术</a></h3>
   	<p>通信/网络设备<em class="vline"></em>500-999人</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/424608.html" ka="search_list_company_5_custompage" target="_blank">东蓝</a></h3>
   	<p>计算机软件<em class="vline"></em>20-99人</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/113231.html" ka="search_list_company_6_custompage" target="_blank">浙江金妮儿</a></h3>
   	<p>移动互联网<em class="vline"></em>B轮<em class="vline"></em>100-499人</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/1179487.html" ka="search_list_company_7_custompage" target="_blank">金薇</a></h3>
   	<p>互联网<em class="vline"></em>100-499人</p>
   </div><div class="company-text">
   	<h3 class="name"><a href="/gongsi/1671813.html" ka="search_list_company_8_custompage" target="_blank">悬剑网络科技有限公司</a></h3>
   	<p>信息安全<em class="vline"></em>0-20人</p>
   </div>
   ```

10. 现在已经有很多数据了，接下来都要考虑怎么保存的问题了。是时候开始研究item.py的作用了。

  >Item 对象是种简单的容器，保存了爬取到得数据。其提供了[类似于词典(dictionary-like)的API](http://docs.python.org/library/stdtypes.html#dict)以及用于声明可用字段的简单语法。

11. 我在不十分理解item的情况下按照示例写了我需要的item

    ```python
    # -*- coding: utf-8 -*-

    # Define here the models for your scraped items
    #
    # See documentation in:
    # https://doc.scrapy.org/en/latest/topics/items.html

    import scrapy
    class BossGrabbingItem(scrapy.Item):
        # define the fields for your item here like:
        name = scrapy.Field()#等号左边就是我们要定义的字段
        url = scrapy.Field()
        area = scrapy.Field()
        size = scrapy.Field()
        finance = scrapy.Field()
        description = scrapy.Field()
        pass

    ```

    等号左边就是我们要定义的字段。

12. 然后在`company.py`的`parse()`方法中组装item

    ```python
       def parse(self, response):
            item = BossGrabbingItem()
            titles = response.xpath("//div[@class='company-text']").extract()

            for context in titles:
                item['name'] = re.findall('_blank">.*</a>', context)[0][8:-4]
                item['url'] = re.findall('/[^/]*html', context)[0][1:-5]
                item['area'] = re.findall('<p>[^<]*<em', context)[0][3:-3]
                finance = re.findall('em>.*<em', context)
                if len(finance) > 0:
                    item['finance'] = finance[0][3:-3]
                else:
                    item['finance'] = ''
                item['size'] = re.findall('em>[^<]*</p', context)[0][3:-3]
    		    print(item['name']+":"+item['url']+":"+item['area']+""+item['size']+":"+item['finance'])
    ```

    这时运行后输出是

    ```
    魅洛迪:1148153:计算机软件:0-20人:

    天广汇通:872756:数据服务:100-499人:

    宁波普天信息技术:1714884:通信/网络设备:500-999人:

    AmarSoft:566493:互联网金融:1000-9999人:已上市

    奥林科技:33637:互联网:100-499人:A轮

    东蓝:424608:计算机软件:20-99人:

    浙江金妮儿:113231:移动互联网:100-499人:B轮

    南边陀螺:1440691:计算机服务:0-20人:天使轮

    腾云互联:1742286:移动互联网:20-99人:不需要融资

    金薇:1179487:互联网:100-499人:

    牛吧科技:1634553:计算机软件:0-20人:不需要融资
    ```

13. 现在已经十分接近我想要的内容了，现在要学习怎么保存数据了。一开始想着是通过文本存储，但是文本的去重是个麻烦事，远没有操作数据库来得简单。所以最后还是采用sqlite3进行存储。因为python自带sqlite3的库😄

14. 网上边搜边学，写出来了`pipeline.py`，写完后记得在`setting.py`中打开这段注释

    ```python
    # Configure item pipelines
    # See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
    ITEM_PIPELINES = {
       'boss_grabbing.pipelines.BossGrabbingPipeline': 300,#这个貌似在多个pipeline时才有效果，数值小的优先
    }
    ```

    下面就是`pipeline.py`

    ```python
    # -*- coding: utf-8 -*-

    # Define your item pipelines here
    #
    # Don't forget to add your pipeline to the ITEM_PIPELINES setting
    # See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html
    import os

    import sqlite3
    class BossGrabbingPipeline(object):

        # 打开数据库
        def open_spider(self, spider):
            db_name = spider.settings.get('SQLITE_DB_NAME', 'sqlite3.db')
            self.db_conn = sqlite3.connect(db_name)
            self.db_cur = self.db_conn.cursor()

        # 关闭数据库
        def close_spider(self, spider):
            self.db_conn.commit()
            self.db_conn.close()
    	
        #操作item
        def process_item(self, item, spider):
            print("process")
            count = self.select_db(item)[0][0]#返回的是tuple，实践证明这里是个二维数组
            print("count:" + str(count))
            if count == 0:#不重复的才插入
                self.insert_db(item)
            return item

        # 插入数据
        def insert_db(self, item):
            values = (
                item['url'],
                item['name'],
                item['area'],
                item['finance'],
                item['size'],
            )

            sql = 'INSERT INTO company (id,name,area,finance,size) VALUES(?,?,?,?,?)'
            self.db_cur.execute(sql, values)

        # select数据
        def select_db(self, item):
            values = (
                item['url'],
            )

            sql = 'select count(1) from company where id=?'
            return self.db_cur.execute(sql, values).fetchall()
    ```

15. 对了，数据库我直接通过`sqliteStudio`创建了，并且建了`company`表。现在我开始改写`company.py`的`parse（）`方法

    ```python
        def parse(self, response):
            item = BossGrabbingItem()
            titles = response.xpath("//div[@class='company-text']").extract()
            for context in titles:
                item['name'] = re.findall('_blank">.*</a>', context)[0][8:-4]
                item['url'] = re.findall('/[^/]*html', context)[0][1:-5]
                item['area'] = re.findall('<p>[^<]*<em', context)[0][3:-3]
                finance = re.findall('em>.*<em', context)  ```
                if len(finance) > 0:
                    item['finance'] = finance[0][3:-3]
                else:
                    item['finance'] = ''
                item['size'] = re.findall('em>[^<]*</p', context)[0][3:-3]
                yield item#嗯，这样pipeline就会取处理了
    ```

16. 运行后查看数据库发现已经有记录了，单页的数据抓取已经完成！！接下来就要进行所有页面的数据抓取了。首先确定列表一共有10页，那么就是要查十次嘛。好的，循环构造一下网址。

    ```python
    import scrapy
    from scrapy import Request

    import re

    from boss_grabbing.items import BossGrabbingItem

    class BossSpider(scrapy.Spider):
        # start_urls = ['https://www.zhipin.com/c101210400-p100101/h_101210400/?page=2']
        base_url = 'https://www.zhipin.com/c101210400-p100101/h_101210400/?page='

        # 新实现的方法
        def start_requests(self):
            # 构造网址列表
            for i in range(1, 12):
                url = self.base_url + str(i)
                yield Request(url, self.parse)#它会迭代去抓取

        def parse(self, response):
            item = BossGrabbingItem()
            titles = response.xpath("//div[@class='company-text']").extract()
            for context in titles:
                item['name'] = re.findall('_blank">.*</a>', context)[0][8:-4]
                item['url'] = re.findall('/[^/]*html', context)[0][1:-5]
                item['area'] = re.findall('<p>[^<]*<em', context)[0][3:-3]
                finance = re.findall('em>.*<em', context)
                if len(finance) > 0:
                    item['finance'] = finance[0][3:-3]
                else:
                    item['finance'] = ''
                item['size'] = re.findall('em>[^<]*</p', context)[0][3:-3]
                yield item
    ```

17. 现在，已经得到了一个充满公司的数据表。然而没有公司细节，所以还是要从数据表里select出来再去爬。照着葫芦画瓢，我写一个爬公司详情的文件

    ```python
    import sqlite3
    import re
    import scrapy
    from scrapy import Request

    class BossSpider(scrapy.Spider):
        name = 'bossDetail'
        base_url = 'https://www.zhipin.com/gongsi/'
        con = sqlite3.connect('sqlite3.db')
        cur = con.cursor()
        cur.execute('select * from(SELECT *,rowid FROM company where description is null) '
                    'where rowid < ((select min(rowid) from company where description is null) + 40)')#每次迭代40个公司
        company_list = cur.fetchall()

        # 这是爬虫开始干活的地址，必须是一个可迭代对象。

        def start_requests(self):
            # 构造网址列表
            for i in self.company_list:
                url = self.base_url + str(i[0]) + ".html"
                yield Request(url, self.parse, meta={'id': i[0]})

        def parse(self, response):
            detail_content = response.xpath("//div[@class='detail-content']//div[@class='text fold-text']").extract()
            address_content = response.xpath("//div[@class='detail-content']//div[@class='job-location']").extract()
            print(detail_content)
            print(address_content)
            url = str(response.meta['id'])
            addresses = re.findall("a>[^<]+</div", str(address_content))
            description = ""
            descriptions = re.findall('text\">.+<a', str(detail_content))
            if len(descriptions) > 0:
                description = descriptions[0][6:-3]
            address_list = ""
            for address in addresses:
                address_list = address_list + ";" + address[2:-6]
            self.cur.execute('update company set description=?,address=? where id=?',
                             (description, str(address_list), int(url)))
            self.con.commit()

    ```

18. 同样再在start文件夹下新增`detail.python`，内容为

    ```python
    #!/usr/bin/python
    #coding:utf-8

    from scrapy import cmdline

    cmdline.execute("scrapy crawl bossDetail".split())
    ```

    跑了几次之后发现我的ip已经被封了...所以我想着能不能有个delay设置。网上一查，发现setting.py中有个值可以修改

    ```python
    # Configure a delay for requests for the same website (default: 0)
    # See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay
    # See also autothrottle settings and docs
    DOWNLOAD_DELAY = 3 #把这个注释打开
    ```

    这样就是3秒请求一次，然而我还是不知道封ip的规则，所以分了好几天把公司跑完。

19. 恩，现在已经很完美了。只不过查询列表和查询公司详情要分两次运行，着实有点不优雅，下一步就是合并代码。这时候就得重新考虑一个数据去重的问题，因为多个岗位对应的可能是同一个公司，所以在爬取公司详情之前就要进行判断，以达到减少爬取页面的目的。

20. 这样的话我就要在生成url之前去数据库判断是否存在这家公司。所以我要在spider里操作数据库。参考了一些代码，我将数据库相关操作抽离了出来。在根目录新建`sqlite.py`

    ```python
    import sqlite3
    import time

    con = sqlite3.connect('sqlite3.db')
    cur = con.cursor()
    print("sqlite连接成功")

    class Sqlite(object):
        # 检查公司是否存在
        @classmethod
        def select_db(cls, company_id):
            sql = 'select count(1) from company where id=?'
            return cur.execute(sql, (company_id,)).fetchall()

        @classmethod
        def insert_db(cls, item):
            values = (
                item['url'],
                item['name'],
                item['area'],
                item['finance'],
                item['size'],
                item['description'],
                item['addresses'],
                time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            )

            sql = 'INSERT INTO company (id,name,area,finance,size,DESCRIPTION,ADDRESS,CREATE_TIME) VALUES(?,?,?,?,?,?,?,?)'
            cur.execute(sql, values)
            con.commit()
    ```

21. 然后把`detail.py`中的内容改到`company.py` 中

    ```python
    import scrapy
    from scrapy import Request

    import re

    from boss_grabbing.items import BossGrabbingItem
    from boss_grabbing.sqlite import Sqlite

    class BossSpider(scrapy.Spider):
        # 这里是将爬虫定义为scrapy.Spider这个类下的一个实例。
        # Spider这个类定义了爬虫的很多基本功能，我们直接实例化就好，
        # 省却了很多重写方法的麻烦。
        name = 'bossZhiPin'
        # 这是爬虫的名字，这个非常重要。
        # start_urls = ['https://www.zhipin.com/c101210400-p100101/h_101210400/?page=2']
        base_url = 'https://www.zhipin.com/c101210400-p100101/h_101210400/?page='

        # 这是爬虫开始干活的地址，必须是一个可迭代对象。

        def start_requests(self):
            # 构造网址列表
            for i in range(0, 2):
                url = self.base_url + str(i)
                yield Request(url, self.parse)

        def parse(self, response):
            titles = response.xpath("//div[@class='company-text']").extract()  # xpath的内容网上搜
            # 爬虫收到上面的地址后，就会发送requests请求，在收到服务器返回的内容后，就将内容传递给parse函数。在这里我们重写函数，达到我们想要的功能。
            for context in titles:
                item = BossGrabbingItem()
                item['url'] = re.findall('/[^/]*html', context)[0][1:-5]
                print(item['url'])
                count = Sqlite.select_db(item['url'])[0][0]  # 没有具体研究过，默认返回的是tuple类型，类似二维数组
                if count > 0:
                    return
                item['name'] = re.findall('_blank">.*</a>', context)[0][8:-4]
                item['area'] = re.findall('<p>[^<]*<em', context)[0][3:-3]
                finance = re.findall('em>.*<em', context)
                if len(finance) > 0:
                    item['finance'] = finance[0][3:-3]
                else:
                    item['finance'] = ''
                item['size'] = re.findall('em>[^<]*</p', context)[0][3:-3]
                url = 'https://www.zhipin.com/gongsi/' + item['url'] + ".html"
                yield Request(url, callback=self.get_detail, meta={'item': item})  # 这样就会调用get_detail()方法，并且把item传过去

        # 获取公司的详情和地址
        def get_detail(self, response):
            item = response.meta['item']
            detail_content = response.xpath("//div[@class='detail-content']//div[@class='text fold-text']").extract()
            address_content = response.xpath("//div[@class='detail-content']//div[@class='job-location']").extract()
            print(detail_content)
            print(address_content)
            addresses = re.findall("a>[^<]+</div", str(address_content))
            description = ""
            descriptions = re.findall('text\">.+<a', str(detail_content))
            if len(descriptions) > 0:
                description = descriptions[0][6:-2]
            address_list = ""
            for address in addresses:
                address_list = address_list + ";" + address[2:-5]
            item['description'] = description
            item['addresses'] = address_list[1:]  # 去掉第一个分号
            yield item

    ```

22. 同样，还要修改`pipelines.py`

    ```python
    from boss_grabbing.sqlite import Sqlite

    class BossGrabbingPipeline(object):

        def process_item(self, item, spider):
            print("process")
            count = Sqlite.select_db(item['url'])[0][0]
            print("count:" + str(count))
            if count == 0:
                Sqlite.insert_db(item)
            return item

    ```

23. 运行`boss_start.py`，得到了一个173条记录的数据表。

24. 结束。


